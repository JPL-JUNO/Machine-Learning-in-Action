\chapter{基于概率论的分类方法：朴素贝叶斯\label{chapter04}}
先统计特征在数据集中取某个特定值的次数，然后除以数据集的实例总数，就得到了特征取该值的概率。我们将在此基础上深入讨论。

从一个最简单的概率分类器开始，然后给出一些假设来学习朴素贝叶斯分类器。我们称之为“朴素”，是因为整个形式化过程只做最原始、最简单的假设。
\section{基于贝叶斯决策理论的分类方法\label{4.1}}
\begin{tcolorbox}[title=朴素贝叶斯]
    优点：在数据较少的情况下仍然有效，可以处理多类别问题。\\
    缺点：对于输入数据的准备方式较为敏感。\\
    适用数据类型：标称型数据。
\end{tcolorbox}


\figures{Two probability distribution with known parameters describing the distribution}

假设现在我们有一个数据集，它由两类数据组成，数据分布\autoref{Two probability distribution with known parameters describing the distribution}所示。

假设有位读者找到了描述图中两类数据的统计参数。（暂且不用管如何找到描述这类数据的统计参数，\autoref{chapter10}会详细介绍。）我们现在用$p1(x,y)$表示数据点$(x,y)$属于类别1（图中用圆点表示的类别）的概率，用$p2(x,y)$表示数据点$(x,y)$属于类别2（图中用三角形表示的类别）的概率，那么对于一个新数据点$(x,y)$，可以用下面的规则来判断它的类别：
\begin{itemize}
    \item 如果 $p1(x,y) > p2(x,y)$，那么类别为1。
    \item 如果 $p2(x,y) > p1(x,y)$，那么类别为1。
\end{itemize}
也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即\important{选择具有最高概率的决策}(朴素贝叶斯是贝叶斯决策理论的一部分)。

\begin{tcolorbox}[title=贝叶斯？]
    这里使用的概率解释属于贝叶斯概率理论的范畴，该理论非常流行且效果良好。贝叶斯概率先验知识和逻辑推理来处理不确定命题。另一种概率解释称为频数概率（frequency probability），它只从数据本身获得结论，并不考虑逻辑推理及先验知识。
\end{tcolorbox}

\section{条件概率}

\section{使用条件概率来分类}
\autoref{4.1}提到贝叶斯决策理论要求计算两个概率$p1(x, y)$和$p2(x, y)$，但这两个准则并不是贝叶斯决策理论的所有内容。使用$p1(~)$和$p2(~)$只是为了尽可能简化描述，而真正需要计算和比较的是$p(c1|x, y)$和$p(c2|x, y)$。这些符号所代表的具体意义是：给定某个由$(x, y)$表示的数据点，那么该数据点来自类别$c1$的概率是多少？数据点来自类别$c2$的概率又是多少？具体地，应用贝叶斯准则得到：
\begin{equation*}
    p(c_i|x, \bm{y})=\frac{p(x,\bm{y}|c_i)p(c_i)}{p(x,\bm{y})}
\end{equation*}

使用这些定义，可以定义贝叶斯分类准则为：
\begin{itemize}
    \item 如果$P(c1|x, \bm{y}) > P(c2|x, \bm{y})$，那么属于类别$c1$。
    \item 如果$P(c1|x, \bm{y}) < P(c2|x, \bm{y|})$，那么属于类别$c2$。
\end{itemize}

\section{使用朴素贝叶斯进行文档分类}
机器学习的一个重要应用就是文档的自动分类。在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词的出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。朴素贝叶斯是上节介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。

使用每个词作为特征并观察它们是否出现，这样得到的特征数目会有多少呢？针对的是哪一种人类语言呢？当然不止一种语言。据估计，仅在英语中，单词的总数就有$\num{500000}$之多。为了能进行英文阅读，估计需要掌握数千单词。

要得到好的概率分布，就需要足够的数据样本，假定样本数为$N$。由统计学知，如果每个特征需要$N$个样本，那么对于10个特征将需要$N^{10}$个样本，对于包含1000个特征的词汇表将需要$N^{1000}$个样本。可以看到，所需要的样本数会随着特征数目增大而迅速增长。

如果特征之间相互独立，那么样本数就可以从$N^{1000}$减少到$1000\times N$。所谓独立（independence）指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系。这个假设正是朴素贝叶斯分类器中朴素（naive）一词的含义（\notes{应该是做了条件独立性假设，参考李航《机器学习方法》}）。朴素贝叶斯分类器中的另一个假设是，每个特征同等重要\footnote{朴素贝叶斯分类器通常有两种实现方式：一种基于贝努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。4.5.4节给出的实际上是多项式模型，它考虑词在文档中的出现次数。——译者注}。其实这个假设也有问题。 如果要判断留言板的留言是否得当，那么可能不需要看完所有的1000个单词，而只需要看10～20个特征就足以做出判断了。尽管上述假设存在一些小的瑕疵，但朴素贝叶斯的实际效果却很好。
\section{使用 Python 进行文本分类}
要从文本中获取特征，需要先拆分文本。具体如何做呢？这里的特征是来自文本的词条（token），一个词条是字符的任意组合。可以把词条想象为单词，也可以使用非单词词条，如URL、IP地址或者任意其他字符串。然后将每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档中，0表示词条未出现。

\subsection{准备数据：从文本中构建词向量}
我们将把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。创建一个叫bayes.py的新文件：、
\begin{tcolorbox}
    bayes.py
\end{tcolorbox}

以在线社区的留言板为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类别：侮辱类和非侮辱类，使用1和0分别表示。

\subsection{准备数据：从文本中构建词向量}
函数loadDataSet()创建了一些实验样本。该函数返回的第一个变量是进行词条切分后的文档集合，这些文档来自斑点犬爱好者留言板。这些留言文本被切分成一系列的词条集合，标点符号从文本中去掉。loadDataSet()函数返回的第二个变量是一个类别标签的集合。这里有两类，侮辱性和非侮辱性。这些文本的类别由人工标注，这些标注信息用于训练程序以便自动检测侮辱性留言。

函数createVocabList()会创建一个包含在所有文档中出现的不重复词的列表，为此使用了Python的set数据类型。将词条列表输给set构造函数，set就会返回一个不重复词表。首先，创建一个空集合，然后将每篇文档返回的新词集合添加到该集合中。操作符 \textbar 用于求两个集合的并集，这也是一个按位或（OR）操作符。在数学符号表示上，按位或操作与集合求并操作使用相同记号。

函数setOfWords2Vec()的输入参数为词汇表及某个文档，输出的是文档向量，向量的每一元素为1或0，分别表示词汇表中的单词在输入文档中是否出现。函数首先创建一个和词汇表等长的向量，并将其元素都设置为0。接着，遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1。一切都顺利的话，就不需要检查某个词是否还在vocabList中，后边可能会用到这一操作。

\subsection{训练算法：从词向量计算概率}
前面介绍了如何将一组单词转换为一组数字，接下来看看如何使用这些数字计算概率。现在已经知道一个词是否出现在一篇文档中，也知道该文档所属的类别。我们重写贝叶斯准则，将之前的$x$、$y$ 替换为$\bm{w}$。$\bm{w}$表示这是一个向量，即它由多个数值组成。在这个例子中，数值个数与词汇表中的词个数相同。
\begin{equation*}
    p(c_i|\bm{w})=\frac{p(\bm{w}|c_i)p(c_i)}{p(\bm{w})}
\end{equation*}

我们将使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。如何计算呢？首先可以通过类别$i$（侮辱性留言或非侮辱性留言）中文档数除以总的文档数来计算概率$p(c_i)$。接下来计算$p(\bm{w}|c_i)$，这里就要用到朴素贝叶斯假设。如果将w展开为一个个独立特征，那么就可以将上述概率写作$p(w_0,w_1,w_2\cdots w_N|c_i)$。这里假设所有词都互相独立，该假设也称作条件独立性假设，它意味着可以使用$p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)\cdots p(w_N|c_i)$来计算上述概率，这就极大地简化了计算的过程。

该计算过程的伪代码如下：
\begin{algorithm}
    \caption{条件概率计算（贝叶斯）}
    计算每个类别中的文档数目\;
    \For{每篇训练文档}{
        \For{每个类别}{
            \If{词条出现在文档中}{
                增加该词条的计数值\;
                增加所有词条的计数值\;
            }
        }
        \For{每个类别}{
            \For{每个词条}{
                将该词条的数目除以总词条数目得到条件概率\;
            }
        }
        \Return{每个类别的条件概率}
    }

\end{algorithm}